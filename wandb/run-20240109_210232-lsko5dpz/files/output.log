
Using device: cpu
{'0': 'The earth was formless and empty, and darkness covered the deep waters. And the Spirit of God was hovering over the surface of the waters.   ', '1': 'To, ƙasa dai ba ta da siffa, babu kuma kome a cikinta, duhu ne kawai ya rufe ko’ina, Ruhun Allah kuwa yana yawo a kan ruwan.   '}
Max length of source sentence: 136
Max length of target sentence: 107
The model has 43,917,395 trainable parameters
x_shapetorch.Size([1, 150, 1, 32]) and tensor([[[[-0.1062-7.5716e-01j,  0.8381+8.2480e-01j,  0.6221+3.4013e-01j,
           -0.1149-6.7190e-01j,  1.3939-1.0013e-01j,  0.9114-8.6924e-01j,
           -1.6609+1.8782e+00j, -0.1374-1.0140e+00j, -0.3565-4.3336e-01j,
           -0.4447-1.3649e-01j,  0.1811+4.8636e-01j,  0.3710+1.2040e+00j,
           -0.4483-6.3710e-02j, -0.0894-1.1636e+00j,  0.4263+3.4359e-01j,
            0.7605-1.4002e+00j,  0.2824+6.2774e-01j, -0.4872-1.3833e+00j,
           -0.9935-1.4672e+00j, -1.9289-9.5911e-01j, -0.4668+1.8018e+00j,
           -0.1926+1.1696e+00j, -0.1174-2.1948e+00j, -1.4281+5.9070e-01j,
            0.1498+9.4170e-01j,  0.2399+5.8184e-01j, -0.6624+1.1653e+00j,
            0.0974+2.5458e-01j,  0.7690-8.4633e-01j, -1.0534+2.0215e-01j,
           -0.3228-1.2370e+00j,  0.3649+1.0320e+00j],
          [ 0.2081+2.0228e+00j, -0.8789+8.1121e-01j,  1.6569+4.8294e-01j,
           -0.2794+5.5971e-01j,  0.3275-4.3704e-01j,  0.5719+7.7466e-02j,
            0.6506+1.7749e+00j,  0.4872+1.2659e+00j, -0.2587+5.3506e-01j,
            1.4394-1.3865e+00j, -1.0648+6.1101e-01j,  1.3201-1.5214e+00j,
           -0.6883+8.5695e-02j, -1.1513+4.3065e-01j, -0.3855-1.5855e+00j,
            0.1086+1.8017e-01j, -0.6783-1.8211e+00j, -0.4943-3.3296e-01j,
           -0.4030-1.8626e+00j, -0.8288-9.4795e-01j,  0.4791+8.1049e-01j,
           -0.0941-4.5229e-01j,  2.0938+6.2631e-01j,  0.6693+8.8147e-02j,
           -1.7157+1.6510e+00j, -0.2778-7.1370e-02j,  2.1880+8.2600e-01j,
           -0.4843+7.7939e-01j, -0.7508+1.6946e+00j,  1.0868-3.4757e-01j,
            1.0645+2.5952e-02j,  1.6971+1.5090e+00j],
          [-0.8857+2.1658e-01j, -0.0358+4.4668e-01j, -0.0728+2.7928e-01j,
           -0.6342+2.3525e-01j,  2.0590-2.4203e-01j, -1.1995-5.5400e-01j,
           -1.0232+5.8615e-01j,  0.6611+6.6583e-01j, -0.1519+1.3539e-01j,
            0.5967+1.1388e+00j,  0.5559-1.4716e-01j, -1.7068-6.3036e-02j,
           -0.4697-9.3146e-02j,  3.0340+5.1815e-01j, -1.7082-9.1258e-01j,
           -0.7983-6.2228e-01j, -0.2071-5.6469e-01j, -0.5598-1.4801e+00j,
           -0.2191+5.0854e-01j,  0.4138-5.2700e-01j,  0.8038+3.0293e-01j,
            1.2256-9.3824e-01j, -1.6879-3.8770e-01j, -0.4715+3.4912e-02j,
            0.5394-1.3979e+00j, -0.5935+1.4007e+00j, -1.0545+6.4254e-01j,
            0.4377-7.9028e-01j, -0.6810+9.5988e-01j,  0.9912-2.0864e-01j,
            0.6562-4.6542e-01j,  1.6144+1.6464e+00j],
          [-0.9331-4.8327e-01j,  0.1807-7.2051e-01j, -1.2726-1.2782e+00j,
            0.4800+1.1527e+00j, -0.0484+7.3775e-01j,  0.8688+4.1732e-01j,
            0.7734-9.2389e-01j,  1.6583-1.7609e+00j, -1.3880-7.6527e-02j,
            0.6007-1.3314e+00j, -0.6422-1.3142e+00j, -0.0205+2.8371e+00j,
            0.3266+5.8121e-01j,  0.6336+1.2301e-01j,  1.1237-3.2945e-01j,
            1.8605-1.3421e+00j,  1.4905+6.8094e-01j,  1.4712-9.8951e-01j,
           -0.1217-2.0010e+00j, -0.8035+2.1601e-01j, -0.3401-1.6789e+00j,
            0.1977+1.3975e+00j,  0.1380-5.1648e-01j, -0.5812-6.9438e-01j,
            1.0464-1.2883e+00j,  1.7399+1.7687e-01j, -0.9633+4.0672e-02j,
            1.1571-4.7221e-01j, -1.2617-8.9814e-01j,  0.7737-6.1833e-01j,
            0.1614-9.3830e-01j,  0.2506+9.1162e-01j],
          [ 0.3073-1.0226e+00j,  0.0577-2.9994e-01j,  1.6101-1.5262e+00j,
            0.7601-2.4884e-01j, -0.2861+8.5803e-01j,  0.8269-2.6594e-01j,
           -0.5029+4.1649e-01j,  0.0194+5.5877e-01j, -0.0061+1.7421e+00j,
           -0.8389-1.3469e+00j,  0.5129-9.8327e-01j, -0.1243-3.4515e-01j,
            1.4728-9.7614e-01j,  0.6765-6.6836e-01j,  0.0910+9.5692e-02j,
           -0.3409+5.2455e-01j,  0.9420-5.0589e-01j,  1.3689+1.2242e+00j,
           -1.9218-3.3372e-01j, -0.2993+1.5660e+00j, -1.1141-9.8291e-01j,
            1.3076+3.3427e-01j,  1.2450-6.8398e-01j, -0.9893-1.4975e+00j,
            0.2134-4.6854e-02j, -0.0588+3.5548e-01j,  1.3072-4.9700e-02j,
            0.6864-5.7166e-01j, -0.9000+1.0371e+00j, -0.8041+1.5344e-01j,
            0.7283-1.2864e+00j, -0.9023-1.5090e-01j],
          [ 0.7971+2.5737e-01j, -1.5923+1.1928e+00j,  1.2219-3.6472e-01j,
           -0.2691+3.2412e-02j,  0.3164+1.0601e+00j,  1.5518-2.8956e-01j,
            1.8244+5.7828e-04j, -0.8796+1.0076e+00j, -0.4671+1.2266e+00j,
            1.9112-1.2833e+00j, -1.5476+1.6097e-02j, -0.4141-3.5423e-01j,
           -2.0122+1.5621e-01j,  1.0609+2.1621e+00j,  1.2928-3.0120e-01j,
           -0.2292+1.3653e+00j,  1.2951-9.5006e-01j,  0.6565+9.2622e-01j,
           -0.3910-5.5589e-01j, -1.4307-1.1666e-01j, -0.7946-8.9268e-02j,
            1.7646-5.5641e-01j,  0.3847-2.0827e-01j, -0.8571+4.1858e-01j,
           -0.3705-9.8609e-01j, -0.4625-8.7983e-01j,  0.3348-9.2048e-01j,
           -2.2149+1.2494e-01j,  0.2264-7.3263e-01j, -1.1663+4.9916e-01j,
           -0.8861+1.4186e+00j,  0.7078-7.0617e-01j],
          [ 0.8058+1.1884e+00j, -2.4808+1.9324e-01j, -0.0695-5.2602e-01j,
            1.7234-1.1601e-01j, -1.5943-1.4072e+00j,  1.9002-1.3033e+00j,
           -0.2897-3.7292e-01j, -0.6129+7.2993e-01j, -0.6269-6.6708e-01j,
            0.5737-1.8457e+00j, -0.2934-4.4258e-01j, -0.2843+8.4265e-01j,
           -0.3076+7.8874e-01j, -1.7994-1.2846e-01j,  0.2022+1.6181e+00j,
           -0.4850-1.7493e-01j,  0.2381-5.6532e-01j, -0.0270+2.3603e-01j,
           -1.3813-2.0560e+00j,  0.6460-1.1365e-01j, -1.7617+8.9886e-01j,
           -0.6861-5.4405e-01j,  0.1027+3.5142e-01j,  0.2230-3.0062e-01j,
           -0.2957-2.9060e-02j,  0.2107-1.3240e-01j,  0.1885-2.5627e-01j,
            0.3902-1.4224e+00j, -1.3076-8.9872e-01j,  0.7909+8.8996e-01j,
            0.1007-1.7083e-01j,  0.0823+2.0925e+00j],
          [ 1.5817+1.3190e+00j,  2.7524+1.5119e+00j,  1.0245+8.4861e-01j,
           -0.0586-7.6647e-01j,  0.8342+4.1229e-01j, -1.2529+4.2457e-02j,
           -1.1169+1.5457e-01j, -3.3774-1.2658e+00j, -0.4319+4.8641e-01j,
           -1.9471+8.1494e-01j, -0.1862+1.2242e+00j, -0.4012+9.5841e-01j,
           -0.5693-2.4051e+00j, -0.8856-3.2696e-01j, -0.5318-1.3788e+00j,
           -1.0163-1.2202e+00j,  1.7086+3.0072e-01j,  0.0841+2.4006e-02j,
            0.1223+1.4195e-01j,  0.8109+9.0385e-01j, -0.1744-3.8662e-01j,
           -1.0332-1.4604e+00j, -0.5928-1.2183e+00j,  0.6529+1.0786e+00j,
            0.4308-8.9471e-01j,  1.4765+5.8233e-01j,  0.2008+1.0697e+00j,
            0.7126-3.0348e-01j,  1.6824-5.1709e-01j,  1.7764-2.2731e+00j,
           -0.5485-1.7458e+00j,  0.3347-8.6879e-01j]],
         [[-1.3389+8.0262e-01j, -0.0606-1.9257e+00j, -0.6908+5.9450e-01j,
           -0.6934+4.0411e-01j,  1.2681+1.3109e+00j, -0.8732+8.2430e-01j,
           -0.0035+6.5328e-01j, -1.4140-3.2654e-01j,  0.2469+2.3649e-01j,
            0.7227-3.9981e-01j, -0.1164-1.3923e-01j,  0.5587-3.6831e-02j,
           -0.9100-1.6063e+00j,  1.7415+2.7456e-01j, -0.1900-5.4266e-01j,
            0.6799-3.7419e-02j,  1.2015-1.5811e-01j, -0.7289-6.0411e-01j,
           -0.1588+4.7910e-01j,  0.6392-1.3334e+00j, -1.9174-1.5980e+00j,
            0.2348+1.1055e-01j,  0.1826+3.3727e-01j,  0.0498+8.3849e-01j,
            0.0588+9.6582e-01j,  0.5728-1.0126e+00j, -0.8759-8.9997e-01j,
            0.9729-4.1799e-01j, -0.8008-1.0341e+00j, -0.7890+5.9168e-01j,
           -0.5003-4.1679e-02j,  0.1539-1.3288e+00j],
          [-1.6910-4.7416e-01j, -0.5255+5.9654e-01j, -1.2892-4.0464e-01j,
           -0.5055-1.3786e+00j,  1.0543+2.6194e-01j, -0.1219-9.2575e-01j,
           -0.2100-1.4868e+00j,  1.3377-9.8783e-02j, -1.3667+2.9240e-01j,
           -1.3829+1.0262e+00j,  0.4777-3.7394e-01j, -0.6228+5.4613e-01j,
           -0.4261-9.4614e-01j, -0.6602-9.6021e-02j, -1.0510-9.6651e-01j,
            0.8064+2.2577e+00j, -0.7155-2.0895e+00j,  0.9642-1.0374e+00j,
            0.2265+1.7509e-01j, -0.1932-9.9634e-02j, -0.1511-1.2602e+00j,
           -0.3262-4.4371e-01j,  0.8419+1.9538e+00j, -0.1421-1.5438e+00j,
            0.1375-8.5759e-01j,  0.9802-1.2538e+00j, -0.0318+9.4042e-01j,
            3.1734-8.4174e-01j, -0.2490-2.4659e-01j,  1.0251-7.0010e-01j,
            1.0178+7.6857e-01j, -1.1735-1.1336e+00j],
          [-0.6196-1.7444e+00j, -0.2479-1.0861e+00j, -1.4608+1.5977e+00j,
            0.5327+1.0901e+00j,  0.6162+5.2235e-01j, -0.1005-1.3184e-01j,
           -0.6474+1.9912e+00j,  0.2407+3.0220e-01j,  0.3866+8.8187e-02j,
            1.0340+1.3146e+00j,  1.0109-6.8611e-01j,  0.4280+1.2545e+00j,
            0.8421-1.3411e+00j,  1.2310+6.8723e-01j, -0.5919+2.1005e-01j,
           -0.2552+1.4893e+00j, -0.1865+1.1679e+00j, -2.2004+1.7537e+00j,
            1.3226-4.2605e-01j, -1.9758-3.5879e-01j, -1.5958+5.2209e-01j,
            0.4962-2.5818e-01j, -0.7719+2.1486e+00j, -2.0374+1.4222e-01j,
           -0.7293+9.7212e-01j, -0.2168-4.0052e-01j, -0.9809-2.5015e-01j,
            0.5821-1.8902e-01j,  0.4201-3.2459e-01j, -0.2985+9.4951e-01j,
           -0.8039+1.4691e+00j,  0.3429-2.8343e-02j],
          [-1.2698-7.1456e-01j,  1.0717+2.9733e+00j, -1.0557+1.9792e+00j,
           -0.2070-4.2368e-01j, -0.3416+7.4856e-01j, -0.9775+4.3072e-01j,
           -0.0048+3.4152e-01j, -0.4030-1.4866e+00j, -1.2103+1.6963e+00j,
           -1.4007+8.4925e-01j,  0.1911+7.5394e-01j, -0.5188-1.1585e+00j,
            1.2972+6.8186e-01j,  0.9332+1.7224e+00j, -0.4930+2.3624e-01j,
            0.6923+3.3752e-01j,  0.6174-1.1165e+00j, -1.5518+4.9446e-01j,
            0.5051+2.5170e+00j,  1.0767-1.8483e-01j,  1.4439+1.5835e+00j,
            0.7164+1.2095e-01j, -0.8180+3.8374e-02j, -0.5098-2.4812e+00j,
           -0.0682+1.7349e+00j,  0.8111+4.7813e-01j,  0.5432+2.1027e-02j,
           -0.6125+6.3790e-01j,  0.9027+1.1967e-01j,  0.7256-7.1046e-01j,
            1.2473-1.2107e+00j, -0.4078-1.9136e-01j],
          [-0.1467-7.7099e-01j, -0.8130+1.2034e-01j, -1.3094+1.8750e-02j,
            0.5831+4.2279e-01j,  0.9345-1.1705e+00j,  0.8027+1.0374e-01j,
            0.6998-6.9031e-01j, -0.9716+8.8306e-01j,  0.1369+8.6511e-01j,
            0.5668-1.1558e-01j, -0.6424-6.7064e-01j,  0.7947-1.2262e+00j,
           -0.5613+1.4677e+00j,  0.1296+1.3374e+00j, -2.5086-5.9192e-01j,
            0.3891+7.2143e-01j, -0.4994+2.1616e+00j, -0.0037-1.5756e+00j,
           -1.0985-4.4488e-01j,  2.0158-1.1274e-01j,  0.8276-5.0524e-02j,
           -0.3867-1.7180e-01j,  1.3194-1.1603e-01j,  0.0183+1.4944e+00j,
            0.8464-9.3842e-01j, -0.2130-2.6315e-01j, -1.6161+4.2625e-01j,
           -1.9023+2.0103e+00j, -0.4258-1.9070e-01j, -0.7122-1.6252e-02j,
           -0.9138-2.8017e-01j, -2.6530+8.0858e-02j],
          [ 1.0812-4.2994e-01j,  0.6900-2.0808e-01j, -1.4673+9.8639e-01j,
           -0.2105-5.5263e-01j,  0.7003+1.0666e+00j, -0.0863+3.9598e-01j,
            0.3422+1.5746e+00j,  0.1917+1.4493e+00j, -0.0919-4.7614e-01j,
            1.0248-1.4666e+00j, -1.2747-8.2767e-01j, -0.0352+2.1930e-02j,
            0.0694-1.0088e+00j, -0.5978+4.2339e-01j, -1.5741+2.9846e-01j,
            0.0677+6.6579e-01j,  1.0965+3.7473e-01j, -0.0671+8.6912e-02j,
           -0.9118+1.6758e-02j, -0.5051+1.9407e-01j,  0.1758-1.4690e+00j,
           -0.9157+4.9733e-01j, -1.8233+7.7396e-01j, -0.9815-1.5783e+00j,
            1.4481+9.7792e-02j,  1.2018+1.2615e+00j,  0.7847-3.3266e-01j,
           -1.1746-2.0413e+00j,  1.1319+2.1582e+00j, -1.1205-5.6167e-01j,
           -0.3034-3.1005e-01j,  2.4257+1.6059e+00j],
          [-0.1684+1.2685e-01j, -0.5395+6.6792e-01j,  0.0570-1.1941e+00j,
            0.3904-5.9308e-01j, -1.2022+4.4079e-01j,  0.0379-1.5658e+00j,
            0.1464+1.1788e+00j, -0.2173+1.0323e-01j,  1.7737-1.7256e+00j,
           -0.3447-8.5141e-01j,  0.1313+7.1136e-02j, -1.2617+4.3516e-01j,
           -0.6238+8.4934e-01j, -0.3164+1.1396e-01j,  0.5981+1.0875e+00j,
           -0.7280-1.1886e+00j, -0.4692+3.7690e-01j, -0.1131+3.5718e-01j,
           -0.2314+1.0845e+00j, -0.0379-5.2470e-01j,  0.5318+9.9710e-01j,
           -1.4746-7.1881e-01j,  0.0925-6.6698e-01j,  0.9390-1.5668e-01j,
            0.8514+1.1605e+00j, -0.1446+3.3651e-01j,  0.7374-4.0615e-02j,
            1.2596+8.6501e-01j, -0.6907+6.7021e-01j, -1.1238+2.3146e-01j,
           -0.9319+5.3059e-01j, -0.0528+6.5768e-02j],
          [ 1.6242-4.2360e-01j,  0.0680+8.3674e-01j, -0.3456+2.6079e+00j,
           -1.0005-6.7338e-01j,  0.2720-1.9885e+00j, -1.8142+7.9070e-01j,
           -1.3694+5.4086e-01j, -1.0252-1.7082e-01j, -1.2774-6.7371e-01j,
            0.4488+5.7980e-01j,  0.1146-1.1656e+00j,  0.0821+3.8245e-01j,
            0.4504+1.6901e+00j, -0.0210-9.0383e-02j,  0.1210-2.4652e-01j,
           -0.0239+7.3230e-01j, -0.3125-1.2712e+00j,  0.5158-1.4894e-01j,
            1.2160+4.3319e-01j,  1.6748-2.9506e-01j, -1.0603+3.6158e-01j,
           -1.7565+8.2512e-01j,  0.8184-3.6034e-01j,  0.0537-1.1957e+00j,
           -1.3122-6.0368e-01j,  1.6653-8.4270e-01j, -0.2101+1.0230e+00j,
           -2.9852-5.5457e-01j, -1.0579+9.5064e-01j, -0.0175-9.6129e-01j,
           -1.4573+9.6548e-01j,  0.0633+1.6849e+00j]]]])
Traceback (most recent call last):
  File "/home/lwasinam/AI_Projects/base_llm/train.py", line 329, in <module>
    train_model(config)
  File "/home/lwasinam/AI_Projects/base_llm/train.py", line 239, in train_model
    run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, 'l', 1)
  File "/home/lwasinam/AI_Projects/base_llm/train.py", line 94, in run_validation
    model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device, target_text)
  File "/home/lwasinam/AI_Projects/base_llm/train.py", line 48, in greedy_decode
    out =model.decode(decoder_input, decoder_mask)
  File "/home/lwasinam/AI_Projects/base_llm/model.py", line 291, in decode
    return self.decoder(x, freqs_complex_form, tgt_mask,)
  File "/home/lwasinam/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/lwasinam/AI_Projects/base_llm/model.py", line 263, in forward
    x = decoder_block(x,freqs_complex_form, tgt_mask)
  File "/home/lwasinam/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/lwasinam/AI_Projects/base_llm/model.py", line 240, in forward
    attention = self.multiheadattention(norm, norm, norm,freqs_complex_form, tgt_mask)
  File "/home/lwasinam/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/lwasinam/AI_Projects/base_llm/model.py", line 163, in forward
    attention = MultiHeadAttention.self_attention(self, query, key, value,freqs_complex_form, mask, self.dropout)
  File "/home/lwasinam/AI_Projects/base_llm/model.py", line 141, in self_attention
    query = apply_rotary_pos_encoding(query, freqs_complex_form, self.device).transpose(2,1)
  File "/home/lwasinam/AI_Projects/base_llm/model.py", line 108, in apply_rotary_pos_encoding
    x_rotated = x_complex * freqs_complex_form
RuntimeError: The size of tensor a (2) must match the size of tensor b (150) at non-singleton dimension 1
Traceback (most recent call last):
  File "/home/lwasinam/AI_Projects/base_llm/train.py", line 329, in <module>
    train_model(config)
  File "/home/lwasinam/AI_Projects/base_llm/train.py", line 239, in train_model
    run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, 'l', 1)
  File "/home/lwasinam/AI_Projects/base_llm/train.py", line 94, in run_validation
    model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device, target_text)
  File "/home/lwasinam/AI_Projects/base_llm/train.py", line 48, in greedy_decode
    out =model.decode(decoder_input, decoder_mask)
  File "/home/lwasinam/AI_Projects/base_llm/model.py", line 291, in decode
    return self.decoder(x, freqs_complex_form, tgt_mask,)
  File "/home/lwasinam/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/lwasinam/AI_Projects/base_llm/model.py", line 263, in forward
    x = decoder_block(x,freqs_complex_form, tgt_mask)
  File "/home/lwasinam/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/lwasinam/AI_Projects/base_llm/model.py", line 240, in forward
    attention = self.multiheadattention(norm, norm, norm,freqs_complex_form, tgt_mask)
  File "/home/lwasinam/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/lwasinam/AI_Projects/base_llm/model.py", line 163, in forward
    attention = MultiHeadAttention.self_attention(self, query, key, value,freqs_complex_form, mask, self.dropout)
  File "/home/lwasinam/AI_Projects/base_llm/model.py", line 141, in self_attention
    query = apply_rotary_pos_encoding(query, freqs_complex_form, self.device).transpose(2,1)
  File "/home/lwasinam/AI_Projects/base_llm/model.py", line 108, in apply_rotary_pos_encoding
    x_rotated = x_complex * freqs_complex_form
RuntimeError: The size of tensor a (2) must match the size of tensor b (150) at non-singleton dimension 1